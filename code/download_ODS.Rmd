---
title: "Acquiring OpenDataScience datasets"
author: "Micha Silver, Arnon Karnieli"
date: "28/02/2021"
output:
  github_document:
    toc: TRUE
    toc_depth: 2
  pdf_document:
    toc: TRUE
    toc_depth: 2
  html_document:
    toc: TRUE
    toc_depth: 2
always_allow_html: true
bibliography: bibliography.bib  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This script demonstrates downloading and clipping high resolution
(30 m pixels) NDVI and Landcover datasets from:

https://maps.opendatascience.eu/

See:

https://opengeohub.medium.com/europe-from-above-space-time-machine-learning-reveals-our-changing-environment-1b05cb7be520

for details of how the datasets were prepared. This archive contains (as of 2021) NDVI as quarterly averages covering 20 years, and landcover (based on both Corine and Landsat) as monthly images, also covering 20 years.

The Tereno site is chosen as a small eLTER area to showcase this data.

## Setup

### Libraries
```{r r libraries, message=FALSE, results='hide', warning=FALSE}
pkg_list = c("terra", "sf", "tmap", "tmaptools", "OpenStreetMap")
installed_packages <- pkg_list %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(pkg_list[!installed_packages], dependencies = TRUE)
}
# Packages loading
pkgs = lapply(pkg_list, library, character.only = TRUE)
```

### Define directories

  * Directories
  * A list of all datasets available from OpenDataScience
  * Select only the NDVI datasets
  
```{r directories, message=FALSE, error=FALSE}
# Edit below as necessary: GIS, output, and figures directories
# and read options and sites files
GIS_dir = "../GIS"
if (!dir.exists(GIS_dir)) {dir.create(GIS_dir,
                                      recursive = TRUE)}

# Where to save outputs
Output_dir = "../Output"
if (!dir.exists(Output_dir)) {dir.create(Output_dir,
                                         recursive = TRUE)}

# List of available ODC rasters, with file names
# Download from: 
raster_list_url <- "https://gitlab.com/geoharmonizer_inea/eumap/-/raw/master/gh_raster_layers.csv?inline=false"
ODS_list_file = "gh_raster_layers.csv"
download.file(url = raster_list_url, destfile = ODS_list_file)
ODS_list = read.csv(ODS_list_file, sep = ";")

# Subset list of NDVI only
ODS_NDVI_list = ODS_list[grepl("ndvi",
                               ODS_list$name, fixed = TRUE),]

# (Add option to ignore Datum unknown warnings)
options("rgdal_show_exportToProj4_warnings"="none")
```

### Load only Tereno site polygon

Load only the Tereno polygon, and transform to the European CRS: ETRS89

```{r load-tereno}
site = "Tereno"
site_gpkg = paste0(site, ".gpkg")
site_sf = read_sf(file.path(GIS_dir, site_gpkg))
# Transform to ETRS89 CRS to match OpenDataScience layers
site_sf_ETRS = st_transform(site_sf, 3035)

# Where to save High resolution outputs
Highres_dir = file.path(Output_dir, site, "NDVI_Highres")
if (!dir.exists(Highres_dir)) {dir.create(Highres_dir,
                                          recursive = TRUE)}
```

## Read all NDVI rasters

  * Read NDVI rasters for 20 years. The "/vsicurl/" virtual file system allows reading only the metadata, thus avoiding download of full (very large) datasets.
  * Thus the URL for download is constructed by concatenating:
    - "/vsicurl/"
    - "https://s3.eu-central-1.wasabisys.com/" (the base URL)
    - "eumap/lcv/" (dataset group/subdirectory)
    - "lcv_ndvi_landsat.glad.ard_p50_30m_0..0cm_200003_eumap_epsg3035_v1.0.tif" (each actual **tif** filename)

  * Crop to Tereno bounding box
  * Save as Geotiff each 3-month average NDVI for 20 years covering the eLTER site

This code runs for a few minutes

```{r read-crop-rasters, eval=FALSE, message=FALSE, warning=FALSE}
ODS_wasabi = "https://s3.eu-central-1.wasabisys.com/"

# Loop thru list of NDVI rasters from OpenDataScience site
lapply(1:length(ODS_NDVI_list$name), FUN = function(x){
  ods_folder = ODS_NDVI_list$folder[x]
  ods_name = ODS_NDVI_list$name[x]
  
  # Extract the year-month from tif name.
  # Use this string to save new Geotiff
  yrmo = unlist(strsplit(ods_name, split = "_", fixed = TRUE))[7]
  yrmo = paste(substr(yrmo, 5, 6), substr(yrmo, 1, 4), sep="-")
  site_tif = paste(site, "NDVI", "Hires", yrmo, sep="_")
  site_path = file.path(Highres_dir, paste0(site_tif, ".tif"))
  
  ods_url = paste0("/vsicurl/",
                   ODS_wasabi, ods_folder, "/", ods_name)
  rast_ndvi = terra::rast(ods_url)
  site_ndvi = crop(rast_ndvi, vect(site_sf_ETRS))
  terra::writeRaster(site_ndvi, site_path, overwrite = TRUE)
})
```

## Visualize

Show two sample NDVI maps

```{r visualization, error=FALSE, warning=FALSE,message=FALSE, fig.width = 5, fig.align = "center"}
# Get all NDVI rasters into terra SpatRaster structure
tif_list = list.files(Highres_dir, pattern = ".tif$",
                      full.names = TRUE)
site_NDVI_stk = rast(tif_list)

tmap_mode("plot")
# read OSM raster data
osm_GER <- read_osm(st_bbox(site_sf),
                    type = "esri-topo", ext=1.2)

# Plot two sample seasons:
for (s in c(13, 15)) {
  site_NDVI = site_NDVI_stk[[s]]
  yrmo = unlist(strsplit(names(site_NDVI), split = "_", fixed = TRUE))[7]
  yrmo = paste(substr(yrmo, 1, 4), substr(yrmo, 5, 6), sep="-")
  ttl = paste(site, "NDVI", yrmo)
  
  m = tm_shape(osm_GER) +
    tm_rgb() +
    tm_shape(site_NDVI) +
      tm_raster(palette = "RdYlGn", alpha = 0.6) +
    tm_shape(site_sf) +
      tm_borders(col="darkgrey", lwd = 1.5) +
    tm_layout(main.title = ttl) +
      tm_scale_bar(position = c("right", "bottom"))
  
  print(m)
  cat("\n")
}

```